[["index.html", "My portfolio Chapter 1 Introduction", " My portfolio Claudia van der Zijden 2021-06-21 Chapter 1 Introduction Welkom to my portfolio! In my portfolio I try to give an impression of my programming skills. This is mainly in r but I also have some experience with bash. This portfolio contains a number of chapters with assignments that I have made, it also contains my resume. The last chapter called machine learning is about a tutorial assignment in which I tried to learn more about machine learning. I hope this portfolio will give you a good idea of my skills. For further questions you can always email claudiavanderzijden@hotmail.nl "],["reproducible-research.html", "Chapter 2 Reproducible research", " Chapter 2 Reproducible research C. elegans plate experiment The data for this exercise was kindly supplied by J. Louter (INT/ILC) and was derived from an experiment in which adult C.elegans nematodes were exposed to varying concentrations of different compounds. The variables RawData (the outcome - number of offspring counted as an integer value, after incubation time), compName (the generic name of the compound/chemical), the compConcentration (the concentration of the compound), and the expType are the most important variables in this dataset. A typical analysis with this data would be to run a dose-response analysis using a log-logistic model with estimates for the maximal, the minimal, the IC50 concentration and the slope at IC50. We will not go into the details but a good package to run such computations and create graphs in R is the {drc} package. See: and:. In the exercise below we will create some visualizations using {ggplot2}. Before we start, we will inspect the dataset. We do this by opening it in Excel. When you look at this dataset, a few things stand out. Among other things, there are many tabs with very large tables without an explanation. This makes it difficult for outsiders to use this data. Then we will load the data into rstudio. library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.4 v purrr 0.3.4 ## v tibble 3.1.2 v dplyr 1.0.7 ## v tidyr 1.1.3 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.1 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(readxl) ce_liq_flow_062 &lt;- read_excel(&quot;data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;, sheet = 1) Now we can look at the data types. we will do this for the columns rawData, compName and compConcentration. typeof(ce_liq_flow_062$RawData) ## [1] &quot;double&quot; typeof(ce_liq_flow_062$compName) ## [1] &quot;character&quot; typeof(ce_liq_flow_062$compConcentration) ## [1] &quot;character&quot; You would expect comConcentration to be numeric but as you can see this is character. Now we are going to make a scatter plot of the data. We put compconcentration on the x-axis and DataRaw on the y-axis. We give a different color to the levels of compname and a different shape to the levels of expType. In addition, we ensure that the numbers below the x-axis are rotated 45 degrees so that we can read those. ggplot(data = ce_liq_flow_062, aes(x = compConcentration, y = RawData)) + geom_point(aes(colour = compName, shape = expType)) + scale_x_discrete(guide = guide_axis(angle = 45)) + labs(title = &quot;compConcentration is double&quot;) ## Warning: Removed 5 rows containing missing values (geom_point). If we now look at this plot, you can see that the scale of the x-axis is not linearly distributed. This is probably due to the data type of comcondition. So were going to change it to numeric. Then we will plot the data again. We now use a log10 transformation to improve the distribution of the x-axis. We also use jitter to avoid overlapping data points. ce_liq_flow_062$compConcentration &lt;- as.numeric(as.character(ce_liq_flow_062$compConcentration)) ## Warning: NAs introduced by coercion typeof(ce_liq_flow_062$compConcentration) ## [1] &quot;double&quot; log10_scatter &lt;-ggplot(data = ce_liq_flow_062, aes(x = compConcentration, y = RawData)) + geom_point(position=position_jitter(width=.1,height=0),aes(colour = compName, shape = compName)) + scale_x_discrete(guide = guide_axis(angle = 45))+ labs(title = &quot;compConcentration is numeric&quot;) log10_scatter + scale_x_log10() ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will ## replace the existing scale. ## Warning: Transformation introduced infinite values in continuous x-axis ## Warning: Removed 36 rows containing missing values (geom_point). The positive control for this experiments is naphthale. The negative control for this experiment is S-medium. After reviewing the data, we could proceed with the analysis of the data to find out whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve. To find out, first check whether the data is normally distributed. This can be done with the shapio-wilk test. This can be used to determine whether a parametric or non-parametric test can be used to see if there is a statistically significant difference between the different groups. Finaly we normalize the data for the controlNegative in such a way that the mean value for controlNegative is exactly equal to 1 and all other values are expressed as a fraction thereof. Than we rerun the graph with the normalized data. normalize &lt;- function(x) { return ((x - min(x)) / (max(x) - min(x))) } library(dplyr) negative &lt;- dplyr::filter(ce_liq_flow_062, expType == &quot;controlNegative&quot;) wat &lt;- normalize(negative$compConcentration) But because of the compConcentration of the negatve control sample is already 0 nothing will change. "],["open-peer-review.html", "Chapter 3 Open Peer Review 3.1 Peer revieuw part 1 3.2 Open peer review part 2", " Chapter 3 Open Peer Review In this assisgment w arw goning to find a scientific article ourself, using PubMed or another database or repository. Well use only Open Access articles. When we fond a article we will go over the Transparency Criteria. This is the link to the article I use https://www.biorxiv.org/content/10.1101/2020.10.02.322917v2.full The title of this article is: Leveraging high-throughput screening data and conditional generative adversarial networks to advance predictive toxicology The authors of this article are: Adrian J. Green, Martin J. Mohlenkamp, Jhuma Das, et al. 3.1 Peer revieuw part 1 Study Purpose : the summary briefly explains what is more important to conduct this research Data Availability Statement: not present Data Location: it does describe what the data should look like and there are references to articles that describe how the data was collected. It does not say where you can find the used data. Study Location: there is no information about where the study was conducted in the material and method section Author Review: the details of the authors are not easy to obtain, the names of the authors are at the top of the article but further contact details are not on the page itself. Ethics Statement: the introduction briefly mentions ethics Funding Statement: nothing is said about funding Code Availability: no code is shared in the article 3.2 Open peer review part 2 Next we are going to try to find a article with R code. We do this on the OSF website. We are going to try to get the code working in our r studio. This is the link to the code we will use https://osf.io/gkcn7/ To make this code work we only have to change the way to load the data, out commend the effect sizes and we have to change the column names so the names are not separated by points. You can find the working script in the appendix, chapther 11 It took little effort to get this script working. On a scale of 0 to 5 I would give it a 4 "],["guerrilla-analytics.html", "Chapter 4 Guerrilla analytics 4.1 Daur2 project 4.2 Portfolio project 4.3 Project project", " Chapter 4 Guerrilla analytics In this assignment I cleaned up my projects according to the Guerrilla analytics.The result can be seen below onder. 4.1 Daur2 project Daur 4.2 Portfolio project Portfolio 4.3 Project project Project "],["curriculum-vitae.html", "Chapter 5 Curriculum vitae", " Chapter 5 Curriculum vitae CV "],["mendaly.html", "Chapter 6 Mendaly", " Chapter 6 Mendaly In practice, many use has been made of RNA sequencing (RNA-seq) methods. With RNA-seq, the transcriptional activity of cells can be examined. Next-generation sequencing is used for this. This results in transcriptomics datasets, which makes it difficult for many researchers to process this data and to draw conclusions from it. (Marini, Linke, and Binder 2020). That is why in this project we are working on a way in which researchers without programming knowledge can easily process transcriptome data. The aim is to develop a shiny app where only a GSE/SRA number needs to be entered, after which an analysis is performed automatically. This saves researchers a lot of time. To make this possible, we use salmon and DESeq2 among others. A GSE number can be used to search the SRA database for a dataset in the form of a fastq file. This fastq file can be used in Salmon. Salmon is a package that quantifies the data.(Patro et al. 2017) . When quantifying the data, it is determined how many of the parts of the data correspond to the reference data. Because Salmon takes small pieces of the data for testing, it works much faster and takes much less memory to perform this analysis than other methods. The analyzed data that then comes from the salmon analysis will be further investigated using deseq2. The DESeq2 package provides methods to test for differential expression using negative binomial generalized linear models. (rnaseq?) A sumerised experiment is then made of this data. (sumexp?) A sumerised experiment is a container where data is stored in different dimensions. It contains row data, it contains information about the genes, and the cold data also contains information about the samples. Finally, the sumerised experiment also contains the number of counts. A sumerised experiment will serve as input for the ISEE shiny app. This ISEE app has been specially developed for analyzing transcriptome data (Lun et al. 2018) In this app not only the count data are processed, but also the row data and the cold data are displayed in graphs. This makes the analysis very comprehensive. In our project we also want to pay attention to a new shiny app. Although the ISEE app is a well-functioning app, there are a number of things that we miss. For example, we think that the app is not clear enough because all graphs are plotted on one page without further additions. In addition, we miss a way in which you can enter the GSE/SRA number. So you still need some programming skills to use this app. Ultimately, it would be nice if you only had to fill in a dataset and you would then receive a message as soon as the analysis is ready. At the moment this analysis is ready, it would be nice if a ready-made report could be downloaded with the result processing and statistical tests in it. Refrences "],["relational-databases.html", "Chapter 7 Relational databases", " Chapter 7 Relational databases In this chapter we are going to get acquainted with sql and dbever. to start, we first load 3 different datasets into Rstudio, namely the flu, dengue and gapminder dataset. And we will make them tidy. library(tidyverse) library(dslabs) ## ## Attaching package: &#39;dslabs&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## gapminder gapminder &lt;- as_tibble(gapminder) flu_data&lt;- read.csv(url(&quot;https://raw.githubusercontent.com/ClaudiavdZ/tlsc-dsfb26v-20_workflows/main/data/flu_data.csv&quot;), skip = 11) flu_data &lt;- as_tibble(flu_data) dengue_data&lt;- read.csv(url(&quot;https://raw.githubusercontent.com/ClaudiavdZ/tlsc-dsfb26v-20_workflows/main/data/dengue_data.csv&quot;), skip = 11) Next we will store them as a csv and RDS file write.table(dengue_data , file = &quot;dengu_data.csv&quot;) write.table(dengue_data , file = &quot;dengu_data.RDS&quot;) write.table(flu_data , file = &quot;flu_data.csv&quot;) write.table(flu_data , file = &quot;flu_data.RDS&quot;) write.table(gapminder , file = &quot;gapminder.csv&quot;) write.table(gapminder , file = &quot;gapminder.RDS&quot;) Then we are goning to watch te data in Dbever library(DBI) con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;myfirstdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;Veroni36&quot;) dbListTables(con) ## [1] &quot;test&quot; &quot;gapminder&quot; &quot;flu_data&quot; &quot;dengue_data&quot; Here you can see what that looks like Dbever Then we will try to put the datasets together as one dataset. flu_usd &lt;- gather( flu_data, key = &quot;country&quot;, value = &quot;flu&quot;, Argentina:Uruguay ) #seperate year from month and day flu_usd &lt;- separate(flu_usd, Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) #count sum of flu flu_usd &lt;- aggregate(flu_usd$flu, by=list(year=flu_usd$year, country=flu_usd$country), FUN=sum) flu_usd &lt;- flu_usd %&gt;% rename(flu = x) flu_usd$year &lt;- as.integer(flu_usd$year) dengue_usd &lt;- gather( dengue_data, key = &quot;country&quot;, value = &quot;dengue&quot;, Argentina:Venezuela ) dengue_usd &lt;- separate(dengue_usd, Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) dengue_usd &lt;- aggregate(dengue_usd$dengue, by=list(year=dengue_usd$year, country=dengue_usd$country), FUN=sum) dengue_usd &lt;- dengue_usd %&gt;% rename(dengue = x) dengue_usd$year &lt;- as.integer(dengue_usd$year) alltogether &lt;- left_join(flu_usd, gapminder, by = c(&quot;country&quot;, &quot;year&quot;)) alltogether &lt;- left_join(alltogether, dengue_usd , by = c(&quot;country&quot;, &quot;year&quot;)) After thad we make some visualisations of the data flu_plot &lt;- function(dataframe, land){ dataframe %&gt;% filter(country == land) %&gt;% ggplot(aes(x = year, y = flu)) + geom_line() + geom_point() } flu_plot(alltogether,&quot;Netherlands&quot;) ## Warning: Removed 2 row(s) containing missing values (geom_path). ## Warning: Removed 2 rows containing missing values (geom_point). alltogether %&gt;% filter(country == &quot;Argentina&quot;) %&gt;% ggplot() + geom_line(aes(y = dengue,x=year, colour = &quot;green&quot;),) + geom_line(aes(y = flu,x=year, colour = &quot;red&quot;)) ## Warning: Removed 2 row(s) containing missing values (geom_path). ## Warning: Removed 2 row(s) containing missing values (geom_path). ggplot(data = alltogether, aes(x = continent, y = flu)) + geom_boxplot(aes(fill = continent)) ## Warning: Removed 72 rows containing non-finite values (stat_boxplot). shapiro.test(alltogether$fertility) ## ## Shapiro-Wilk normality test ## ## data: alltogether$fertility ## W = 0.84528, p-value &lt; 2.2e-16 shapiro.test(alltogether$flu) ## ## Shapiro-Wilk normality test ## ## data: alltogether$flu ## W = 0.70363, p-value &lt; 2.2e-16 shapiro.test(alltogether$dengue) ## ## Shapiro-Wilk normality test ## ## data: alltogether$dengue ## W = 0.91218, p-value = 0.0009743 shapiro.test(alltogether$infant_mortality) ## ## Shapiro-Wilk normality test ## ## data: alltogether$infant_mortality ## W = 0.75988, p-value &lt; 2.2e-16 shapiro.test(alltogether$life_expectancy) ## ## Shapiro-Wilk normality test ## ## data: alltogether$life_expectancy ## W = 0.93264, p-value = 9.214e-12 shapiro.test(alltogether$gdp) ## ## Shapiro-Wilk normality test ## ## data: alltogether$gdp ## W = 0.53327, p-value &lt; 2.2e-16 shapiro.test(alltogether$population) ## ## Shapiro-Wilk normality test ## ## data: alltogether$population ## W = 0.74646, p-value &lt; 2.2e-16 library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some leveneTest(alltogether$fertility, alltogether$flu, center = mean) ## Warning in leveneTest.default(alltogether$fertility, alltogether$flu, center = ## mean): alltogether$flu coerced to factor. ## Warning in anova.lm(lm(resp ~ group)): ANOVA F-tests on an essentially perfect ## fit are unreliable ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 301 1.6735e+26 6.163e-14 *** ## 1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #no equal variance leveneTest(alltogether$dengue, alltogether$infant_mortality, center = mean) ## Warning in leveneTest.default(alltogether$dengue, ## alltogether$infant_mortality, : alltogether$infant_mortality coerced to factor. ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 44 19.715 0.0002092 *** ## 7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #no equal variance leveneTest(alltogether$population, alltogether$life_expectancy, center = mean) ## Warning in leveneTest.default(alltogether$population, ## alltogether$life_expectancy, : alltogether$life_expectancy coerced to factor. ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 137 3.0645 4.125e-14 *** ## 226 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #no equal variance "],["my-own-package.html", "Chapter 8 My own package", " Chapter 8 My own package I also made my own package, my package contains funtions i used for this project. The package is called bestpackage. It has a script to find attributes in dataset, in the biomart database given a search string. A script to make a line graph of flu data A script to make a model And a script to normalise data You can find more in formation about the package on its github: https://github.com/ClaudiavdZ/bestpackage "],["parameters.html", "Chapter 9 Parameters", " Chapter 9 Parameters data &lt;- read.csv(&quot;data/parameter/data.csv&quot;) library(tidyverse) ggplot(data, aes(x = month, y = deaths))+ geom_line(aes(colour = countriesAndTerritories)) ggplot(data, aes(x = month, y = cases))+ geom_line(aes(colour = countriesAndTerritories)) "],["looking-ahead.html", "Chapter 10 Looking ahead", " Chapter 10 Looking ahead I hope to graduate from my life science degree in the summer of 2022. I will start my graduation internship in September. During these internships I hope to gain more experience with zoology but also with programming. In the future I think it would be fun to work on a way to improve animal welfare, during animal experiments but also in livestock farming. For this reason I have delved further into machine learning because I believe that with machine learning many animal experiments will become superfluous. Thats why I first looked into what it is and what it can be used for. I then worked on an assignment myself in which I try to train an algorithm to distinguish two different horse breeds. I used images of two different breeds for this, and trained the algorithm with supervised learning. that is, the data is labeled. "],["machine-learning.html", "Chapter 11 Machine learning", " Chapter 11 Machine learning This chapter contains my self-study assignment which I explained in the previous chapter. Im going to try to train the algorithm in such a way that it can distinguish the Friesian breed from the Fjord breed. First I have to collect the data. I have collect the pictures from google, and i have select them by hand. Than I load them in rstudio an cut them so they are all the same size. # Load libraries library(jpeg) library(EBImage) ## ## Attaching package: &#39;EBImage&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose #Paths to images fjordpath = &quot;./ExtraOpdracht/Fjord&quot; friespath = &quot;./ExtraOpdracht/Fries&quot; #Paths to cut images newfjordpath = &quot;./ExtraOpdracht/FjordC&quot; newfriespath = &quot;./ExtraOpdracht/FriesC&quot; #list to file names filenames = list.files(fjordpath) #loop through filenames for (name in filenames) { # Ignore errors (not jpg files) tryCatch({ # Read images image = readJPEG(paste(fjordpath,name, sep=&quot;/&quot;)) # Check minimum dimension and cut image according to the dimension mindim = min(dim(image)[1:2]) newimage = image[1:mindim,1:mindim,1:3] newimage = resize(newimage, w=100, h=100) # Write cut image writeJPEG(newimage, paste(newfjordpath, name, sep=&quot;/cut_&quot;)) }, error=function(e){}) } # same but for friesian horse images filenames = list.files(friespath) for (name in filenames) { tryCatch({ image = readJPEG(paste(friespath,name, sep=&quot;/&quot;)) mindim = min(dim(image)[1:2]) newimage = image[1:mindim,1:mindim,1:3] newimage = resize(newimage, w=100, h=100) writeJPEG(newimage, paste(newfriespath, name, sep=&quot;/cut_&quot;)) }, error=function(e){}) } As you can see below the images are cut and the same side now. Now we are going to make and test model. #devtools::install_github(&quot;rstudio/reticulate&quot;) #tensorflow::install_tensorflow(extra_packages=&#39;pillow&#39;) #tensorflow::install_tensorflow(extra_packages=&#39;SciPy&#39;) library(tidyverse) library(keras) ## ## Attaching package: &#39;keras&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## normalize ## The following object is masked from &#39;package:EBImage&#39;: ## ## normalize library(tensorflow) library(reticulate) #index labels #0=fjord 1=fries label_list &lt;- dir(&quot;data/Machine_learning/Train/&quot;) #rescale size width &lt;- 224 height&lt;- 224 target_size &lt;- c(width, height) rgb &lt;- 3 #color channels path_train &lt;- &quot;data/Machine_learning/Train/&quot; #rescale pixel values between 0-1 and split off 20% of training data used as validation data train_data_gen &lt;- image_data_generator(rescale = 1/255, validation_split = .2) #make batches of the data in the directory (default = 32) #batches are sets of images train_images &lt;- flow_images_from_directory(path_train, train_data_gen, subset = &#39;training&#39;, target_size = target_size, class_mode = &quot;binary&quot;, shuffle=F, classes = label_list, seed = 2021) validation_images &lt;- flow_images_from_directory(path_train, train_data_gen, subset = &#39;validation&#39;, target_size = target_size, class_mode = &quot;binary&quot;, classes = label_list, seed = 2021) #load xception model and removes the input layer https://arxiv.org/abs/1610.02357 mod_base &lt;- application_xception(weights = &#39;imagenet&#39;, include_top = FALSE, input_shape = c(width, height, 3)) #make sure the exception weights don&#39;t change while training, only change the self appended layers freeze_weights(mod_base) #specify model parameters model_function &lt;- function(learning_rate = 0.001, dropoutrate=0.2, n_dense=1024){ k_clear_session() #build model, append layers to exception model model &lt;- keras_model_sequential() %&gt;% mod_base %&gt;% layer_global_average_pooling_2d() %&gt;% layer_dense(units = n_dense) %&gt;% layer_activation(&quot;relu&quot;) %&gt;% layer_dropout(dropoutrate) %&gt;% layer_dense(units=1, activation=&quot;sigmoid&quot;) #compile model #use binary_crossentropy loss #use adem optimizer https://arxiv.org/abs/1412.6980 #keep track of accuracy metric model %&gt;% compile( loss = &quot;binary_crossentropy&quot;, optimizer = optimizer_adam(lr = learning_rate), metrics = &quot;accuracy&quot; ) return(model) } #visualize model model &lt;- model_function() model ## Model ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## xception (Functional) (None, 7, 7, 2048) 20861480 ## ________________________________________________________________________________ ## global_average_pooling2d (GlobalAve (None, 2048) 0 ## ________________________________________________________________________________ ## dense_1 (Dense) (None, 1024) 2098176 ## ________________________________________________________________________________ ## activation (Activation) (None, 1024) 0 ## ________________________________________________________________________________ ## dropout (Dropout) (None, 1024) 0 ## ________________________________________________________________________________ ## dense (Dense) (None, 1) 1025 ## ================================================================================ ## Total params: 22,960,681 ## Trainable params: 2,099,201 ## Non-trainable params: 20,861,480 ## ________________________________________________________________________________ batch_size &lt;- 32 #numer of times the network will see all of the images epochs &lt;- 6 #train model hist &lt;- model %&gt;% fit( train_images, steps_per_epoch = train_images$n %/% batch_size, epochs = epochs, validation_data = validation_images, validation_steps = validation_images$n %/% batch_size, verbose = 1 ) path_test &lt;- &quot;data/Machine_learning/Test/&quot; test_data_gen &lt;- image_data_generator(rescale = 1/255) test_images &lt;- flow_images_from_directory(path_test, test_data_gen, target_size = target_size, class_mode = &quot;binary&quot;, classes = label_list, shuffle = F, seed = 2021) #test model model %&gt;% evaluate(test_images, steps = test_images$n) ## loss accuracy ## 0.02047309 1.00000000 test_image &lt;- image_load(&quot;data/Machine_learning/Test/Fries/cut_Fries027.jpg&quot;, target_size = target_size) x &lt;- image_to_array(test_image) x &lt;- array_reshape(x, c(1, dim(x))) x &lt;- x/255 pred &lt;- model %&gt;% predict(x) pred ## [,1] ## [1,] 0.9997392 As you can see the algoritme has it almost completely good.This algorithm is therefore able to classify almost all photos correctly. "],["appendix.html", "Chapter 12 Appendix", " Chapter 12 Appendix This code belongs to chapter 3 I dont let this script run because then I get a error because of the plots are to large R code for: L?pez Steinmetz L.C., Dutto Florio M.A., Leyes C.A., Fong S.B., Rigalli A. &amp; Godoy J.C. Levels and predictors of depression, anxiety, and suicidal risk during COVID-19 pandemic in Argentina: The impacts of quarantine extensions on mental health state. library(tidyverse) library(readxl) # Load the dataset: table&lt;-read_excel(&quot;data/Peer/dataset.xlsx&quot;) summary(table) ###### SUB-TITLE: METHODS &gt; Sample and procedure # SAMPLE N = 1100 # Distribution by sex: table(table$SEX) # Absolute frequencies: Men = 217, Women = 883 prop.table(table(table$SEX))*100 # Percentages: Men = 19.72727%, Women = 80.27273% # Central tendency measures by age (entire sample) # Mean mean(table$AGE) # Age: mean = 31.45273 # Standard deviation (sd) sd(table$AGE) # Age: sd = 11.7824 # Standard error (sem) library(&quot;plotrix&quot;) std.error(table$AGE) # Age: sem = 0.3552526 # median median(table$AGE) # Age: median = 28 ###### SUB-TITLE: METHODS &gt; Data analysis # To test Skewness and Kurtosis # Criteria: range of acceptable values or near to -3 and +3 (Brown, 2006). # Reference: Brown, T. A. (2006). Confirmatory factor analysis for applied research. New York: Guilford Press. library(moments) # DEPRESSION skewness(table$DEPRESSION) # skewness DEPRESSION = 1.014193 kurtosis(table$DEPRESSION) # kurtosis DEPRESSION = 3.789272 table &lt;- rename(table, &quot;ANXIETY_STATE&quot; = &quot;ANXIETY STATE&quot;, &quot;ANXIETY_TRAIT&quot; = &quot;ANXIETY TRAIT&quot;, &quot;SUIC_RISK&quot; = &quot;SUIC RISK&quot;, &quot;SUB_PERIODS&quot; = &quot;SUB PERIODS&quot;, &quot;MENTAL_DISORDER_HISTORY&quot; = &quot;MENTAL DISORDER HISTORY&quot;, &quot;SUIC_ATTEMPT_HISTORY&quot; = &quot;SUIC ATTEMPT HISTORY&quot;, &quot;LIVING_WITH_SOMEBODY&quot; = &quot;LIVING WITH SOMEBODY&quot;, &quot;ECONOMIC_INCOME&quot; = &quot;ECONOMIC INCOME&quot;) # ANXIETY STATE skewness(table$ANXIETY_STATE) # skewness ANXIETY STATE = 0.2010007 kurtosis(table$ANXIETY_STATE) # kurtosis ANXIETY STATE = 2.341017 # ANXIETY TRAIT skewness(table$ANXIETY_TRAIT) # skewness ANXIETY TRAIT = 0.2401163 kurtosis(table$ANXIETY_TRAIT) # kurtosis ANXIETY TRAIT = 2.354038 # SUICIDAL RISK skewness(table$SUIC_RISK) # skewness SUICIDAL RISK = 0.8331517 kurtosis(table$SUIC_RISK) # kurtosis SUICIDAL RISK = 3.193105 # For addresing the first aim, we divided the entire sample into three groups: table(table$SUB_PERIODS) # first quarantine extension (1. EXT POST) = 362 # second and third quarantine extensions (2. EXT POST) = 239 # fourth quarantine extension (3. EXT POST) = 499 ########################################################################################### ######################################### RESULTS ######################################### ########################################################################################### ########################################################################################### ######################################### AIM 1 ########################################### # Load this library for computing effect sizes: library(sjstats) ############################################################################################# ### Differences in specific mental health state indicators by three sub-periods of quarantine # 1. EXT POST = first quarantine extension # 2./3. EXT POST = second/third quarantine extensions # 4. EXT POST = fourth quarantine extension # DEPRESSION anovatempdepr &lt;- aov(table$DEPRESSION~table$SUB_PERIODS) anovatempdepr summary(anovatempdepr) plot(anovatempdepr) pairwise.t.test(x = table$DEPRESSION, g = table$SUB_PERIODS, p.adjust.method = &quot;bonferroni&quot;, pool.sd = TRUE, paired = FALSE, alternative = &quot;two.sided&quot;) # significant differences # 2./3. EXT POST-1. EXT POST p adj 0.018 # 4. EXT POST-1. EXT POST p adj 1.3e-05 #effectsize::cohens_f(anovatempdepr, ci = 0.95, partial = TRUE, type = 1) tapply(table$DEPRESSION,factor(table$SUB_PERIODS),mean) tapply(table$DEPRESSION,factor(table$SUB_PERIODS),std.error) library(gplots) # Figure S1 (Supplementary material) plotmeans(table$DEPRESSION~table$SUB_PERIODS, main=&quot;Fig. S1. Depression by quarantine sub-periods. Mean plot with 95% Confidence Interval&quot;, cex.main = 0.8, ylab = &quot;Depression&quot;, xlab = &quot;Quarantine&#39;s sub periods&quot;) mean(table$DEPRESSION) # mean = 15.69545 std.error(table$DEPRESSION) # std. error = 0.3347087 # Percentage distribution by cutoff score: # non clinically depressed: prop.table(table(table$DEPRESSION&lt;20,table$SUB_PERIODS))*100 # clinically depressed: prop.table(table(table$DEPRESSION&gt;=20,table$SUB_PERIODS))*100 # ANXIETY STATE anovatempanxstate &lt;- aov(table$ANXIETY_STATE~table$SUB_PERIODS) anovatempanxstate summary(anovatempanxstate) plot(anovatempanxstate) pairwise.t.test(x = table$ANXIETY_STATE, g = table$SUB_PERIODS, p.adjust.method = &quot;bonferroni&quot;, pool.sd = TRUE, paired = FALSE, alternative = &quot;two.sided&quot;) # significant differences # 2./3. EXT POST-1. EXT POST p adj 0.0057 # 4. EXT POST-1. EXT POST p adj 0.0038 #effectsize::cohens_f(anovatempanxstate, ci = 0.95, partial = TRUE, type = 1) tapply(table$ANXIETY_STATE,factor(table$SUB_PERIODS),mean) tapply(table$ANXIETY_STATE,factor(table$SUB_PERIODS),std.error) # Figure S2 (Supplementary material) plotmeans(table$ANXIETY_STATE~table$SUB_PERIODS, main=&quot;Fig. S2. State-Anxiety by quarantine sub-periods. Mean plot with 95% Confidence Interval&quot;, cex.main = 0.8, ylab = &quot;State-Anxiety&quot;, xlab = &quot;Quarantine&#39;s sub periods&quot;) mean(table$ANXIETY_STATE) # mean = 31.77545 std.error(table$ANXIETY_STATE) # std. error = 0.436393 # low: prop.table(table(table$ANXIETY_STATE&lt;32,table$SUB_PERIODS))*100 # high: prop.table(table(table$ANXIETY_STATE&gt;=32,table$SUB_PERIODS))*100 # ANXIETY TRAIT anovatempanxtrait &lt;- aov(table$ANXIETY_TRAIT~table$SUB_PERIODS) anovatempanxtrait summary(anovatempanxtrait) plot(anovatempanxtrait) pairwise.t.test(x = table$ANXIETY_TRAIT, g = table$SUB_PERIODS, p.adjust.method = &quot;bonferroni&quot;, pool.sd = TRUE, paired = FALSE, alternative = &quot;two.sided&quot;) # significant differences # 2./3. EXT POST-1. EXT POST p adj 0.046 # 4. EXT POST-1. EXT POST p adj 0.022 #effectsize::cohens_f(anovatempanxtrait, ci = 0.95, partial = TRUE, type = 1) tapply(table$ANXIETY_TRAIT,factor(table$SUB_PERIODS),mean) tapply(table$ANXIETY_TRAIT,factor(table$SUB_PERIODS),std.error) # Figure S3 (Supplementary material) plotmeans(table$ANXIETY_TRAIT~table$SUB_PERIODS, main=&quot;Fig. S3. Trait-Anxiety by quarantine sub-periods. Mean plot with 95% Confidence Interval&quot;, cex.main = 0.8, ylab = &quot;Trait-Anxiety&quot;, xlab = &quot;Quarantine&#39;s sub periods&quot;) mean(table$ANXIETY_TRAIT) # mean = 26.89727 std.error(table$ANXIETY_TRAIT) # std. error = 0.3662711 # low: prop.table(table(table$ANXIETY_TRAIT&lt;27,table$SUB_PERIODS))*100 # high: prop.table(table(table$ANXIETY_TRAIT&gt;=27,table$SUB_PERIODS))*100 # SUICIDAL RISK anovatempsuic &lt;- aov(table$SUIC_RISK~table$SUB_PERIODS) anovatempsuic summary(anovatempsuic) plot(anovatempsuic) pairwise.t.test(x = table$SUIC_RISK, g = table$SUB_PERIODS, p.adjust.method = &quot;bonferroni&quot;, pool.sd = TRUE, paired = FALSE, alternative = &quot;two.sided&quot;) # significant differences # 4. EXT POST-1. EXT POST p adj 0.030 #effectsize::cohens_f(anovatempsuic, ci = 0.95, partial = TRUE, type = 1) tapply(table$SUIC_RISK,factor(table$SUB_PERIODS),mean) tapply(table$SUIC_RISK,factor(table$SUB_PERIODS),std.error) # Figure S4 (Supplementary material) plotmeans(table$SUIC_RISK~table$SUB_PERIODS, main=&quot;Fig. S4. Suicidal risk by quarantine sub-periods. Mean plot with 95% Confidence Interval&quot;, cex.main = 0.8, ylab = &quot;Suicidal risk&quot;, xlab = &quot;Quarantine&#39;s sub periods&quot;) mean(table$SUIC_RISK) # mean = 30.32182 std.error(table$SUIC_RISK) # std. error = 0.4926946 # Percentage distribution by cutoff score: # low: prop.table(table(table$SUIC_RISK&lt;30,table$SUB_PERIODS))*100 # moderate: prop.table(table(table$SUIC_RISK&gt;=30&amp;table$SUIC_RISK&lt;=44,table$SUB_PERIODS))*100 # high: prop.table(table(table$SUIC_RISK&gt;=45,table$SUB_PERIODS))*100 ########################################################################################### ######################################### AIM 2 ########################################### ########################################################################## ## 2) Multiple linear regressions: # We performed stepwise selection (direction = both) using the stepAIC() function from the MASS package. library(MASS) # stepAIC() performs stepwise model selection by exact AIC #### DEPRESSION: # Stepwise Regression fitwith&lt;-lm(DEPRESSION~SEX+AGE+PROVINCE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) stepwith &lt;- stepAIC(fitwith, trace=TRUE,direction=&quot;both&quot;) stepwith stepwith$anova # display results # Stepwise Model Path # Analysis of Deviance Table # Initial Model: DEPRESSION ~ SEX + AGE + PROVINCE + EDUCATION + ECONOMIC_INCOME + LIVING_WITH_SOMEBODY + MENTAL_DISORDER_HISTORY + SUIC_ATTEMPT_HISTORY + SUB_PERIODS # Start: AIC = 4951.51 # Final Model: DEPRESSION ~ SEX + AGE + ECONOMIC_INCOME + MENTAL_DISORDER_HISTORY + SUIC_ATTEMPT_HISTORY + SUB_PERIODS # Stepwith: AIC = 4938.7 summary(stepwith) # 95% Confidence interval of best-fitted model: confint(stepwith) # ERROR RATE of best-fitted model: sigma(stepwith)/mean(table$DEPRESSION) # 0.5989474 # In our multiple regression example, the Residual Standard Error (RSE) or sigma is 9.401 corresponding to 60% error rate. par(mfrow=c(2,2)) # Figure S5 (Supplementary material) plot(stepwith) par(mfrow=c(1,1)) # TABLE 1: # Model 1: INITIAL MODEL: model1&lt;-lm(DEPRESSION~SEX+AGE+PROVINCE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model1) # YES significative p-value &lt; 2.2e-16 # Model 2 eliminates PROVINCE: model2&lt;-lm(DEPRESSION~SEX+AGE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model2) # YES significative p-value &lt; 2.2e-16 # Model 3 eliminates EDUCATION: model3&lt;-lm(DEPRESSION~SEX+AGE+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model3) # YES significative p-value &lt; 2.2e-16 # Model 4 eliminates LIVING_WITH_SOMEBODY: model4&lt;-lm(DEPRESSION~SEX+AGE+ECONOMIC_INCOME+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model4) # YES significative p-value &lt; 2.2e-16 ################ ## Considering the predictors included in the best-fitted model (i.e., stepwith) in this group, we analyzed the smallest possible linear model for each specific MHS indicator by means of all two-predictor combinations. # We performed all-subsets regression using the regsubsets() function from the leaps package. # We analyzed the three best models for two-predictor subset sizes. library(leaps) leapsbestwith&lt;-regsubsets(DEPRESSION~SEX+AGE+ECONOMIC_INCOME+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table,nbest=3) summary(leapsbestwith) # The best two-predictors model was: DEPRESSION ~ AGE + SUIC_ATTEMPT_HISTORY==no plot(leapsbestwith,scale=&quot;r2&quot;) plot(leapsbestwith,scale=&quot;adjr2&quot;) # First: AGE + SUIC_ATTEMPT_HISTORY (no): besttwowithfirst&lt;-lm(DEPRESSION~AGE+SUIC_ATTEMPT_HISTORY,data=table) besttwowithfirst summary(besttwowithfirst) confint(besttwowithfirst) # Second: SEX (woman) + SUIC_ATTEMPT_HISTORY (no): besttwowithsecond&lt;-lm(DEPRESSION~SEX+SUIC_ATTEMPT_HISTORY,data=table) besttwowithsecond summary(besttwowithsecond) confint(besttwowithsecond) # Third: SUIC_ATTEMPT_HISTORY (no) + SUB PERIODS (4. EXT POST): besttwowiththird&lt;-lm(DEPRESSION~SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) besttwowiththird summary(besttwowiththird) confint(besttwowiththird) #### ANXIETY-STATE: # Stepwise Regression fitwith&lt;-lm(ANXIETY_STATE~SEX+AGE+PROVINCE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) stepwith &lt;- stepAIC(fitwith, trace=TRUE,direction=&quot;both&quot;) stepwith stepwith$anova # display results # Stepwise Model Path # Analysis of Deviance Table # Initial Model: ANXIETY_STATE ~ SEX + AGE + PROVINCE + EDUCATION + ECONOMIC_INCOME + LIVING_WITH_SOMEBODY + MENTAL_DISORDER_HISTORY + SUIC_ATTEMPT_HISTORY + SUB_PERIODS # Start: AIC = 5679.05 # Final Model: ANXIETY_STATE ~ SEX + AGE + MENTAL_DISORDER_HISTORY + SUIC_ATTEMPT_HISTORY + SUB_PERIODS # Stepwith: AIC = 5660.25 summary(stepwith) # 95% Confidence interval of best-fitted model: confint(stepwith) # ERROR RATE of best-fitted model: sigma(stepwith)/mean(table$ANXIETY_STATE) # 0.4108702 # In our multiple regression example, the Residual Standard Error (RSE) or sigma is 13.06 corresponding to 41% error rate. par(mfrow=c(2,2)) # Figure S6 (Supplementary material) plot(stepwith) par(mfrow=c(1,1)) # TABLE 1: # Model 1: INITIAL MODEL: model1&lt;-lm(ANXIETY_STATE~SEX+AGE+PROVINCE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model1) # YES significative p-value &lt; 2.2e-16 # Model 2 eliminates PROVINCE: model2&lt;-lm(ANXIETY_STATE~SEX+AGE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model2) # YES significative p-value &lt; 2.2e-16 # Model 3 eliminates EDUCATION: model3&lt;-lm(ANXIETY_STATE~SEX+AGE+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model3) # YES significative p-value &lt; 2.2e-16 # Model 4 eliminates ECONOMIC_INCOME: model4&lt;-lm(ANXIETY_STATE~SEX+AGE+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model4) # YES significative p-value &lt; 2.2e-16 # Model 5 eliminates LIVING_WITH_SOMEBODY: model5&lt;-lm(ANXIETY_STATE~SEX+AGE+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model5) # YES significative p-value &lt; 2.2e-16 ################ ## Considering the predictors included in the best-fitted model (i.e., stepwith) in this group, we analyzed the smallest possible linear model for each specific MHS indicator by means of all two-predictor combinations. # We performed all-subsets regression using the regsubsets() function from the leaps package. # We analyzed the three best models for two-predictor subset sizes. leapsbestwith&lt;-regsubsets(ANXIETY_STATE~SEX+AGE+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table,nbest=3) summary(leapsbestwith) # The best two-predictors model was: ANXIETY_STATE ~ AGE + SUIC_ATTEMPT_HISTORY==no plot(leapsbestwith,scale=&quot;r2&quot;) plot(leapsbestwith,scale=&quot;adjr2&quot;) # First: AGE + SUIC_ATTEMPT_HISTORY (no): besttwowithfirst&lt;-lm(ANXIETY_STATE~AGE+SUIC_ATTEMPT_HISTORY,data=table) besttwowithfirst summary(besttwowithfirst) confint(besttwowithfirst) # Second: MENTAL DISORDER (yes) + SUIC_ATTEMPT_HISTORY (no): besttwowithsecond&lt;-lm(ANXIETY_STATE~MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY,data=table) besttwowithsecond summary(besttwowithsecond) confint(besttwowithsecond) # Third: AGE + MENTAL DISORDER (yes): besttwowiththird&lt;-lm(ANXIETY_STATE~AGE+MENTAL_DISORDER_HISTORY,data=table) besttwowiththird summary(besttwowiththird) confint(besttwowiththird) #### ANXIETY-TRAIT: # Stepwise Regression fitwith&lt;-lm(ANXIETY_TRAIT~SEX+AGE+PROVINCE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) stepwith &lt;- stepAIC(fitwith, trace=TRUE,direction=&quot;both&quot;) stepwith stepwith$anova # display results # Stepwise Model Path # Analysis of Deviance Table # Initial Model: ANXIETY_TRAIT ~ SEX + AGE + PROVINCE + EDUCATION + ECONOMIC_INCOME + LIVING_WITH_SOMEBODY + MENTAL_DISORDER_HISTORY + SUIC_ATTEMPT_HISTORY + SUB_PERIODS # Start: AIC = 5137.82 # Final Model: ANXIETY_TRAIT ~ SEX + AGE + ECONOMIC_INCOME + MENTAL_DISORDER_HISTORY + SUIC_ATTEMPT_HISTORY # Stepwith: AIC = 5123.92 summary(stepwith) # 95% Confidence interval of best-fitted model: confint(stepwith) # ERROR RATE of best-fitted model: sigma(stepwith)/mean(table$ANXIETY_TRAIT) # 0.380548 # In our multiple regression example, the Residual Standard Error (RSE) or sigma is 10.24 corresponding to 38% error rate. par(mfrow=c(2,2)) # Figure S7 (Supplementary material) plot(stepwith) par(mfrow=c(1,1)) # TABLE 1: # Model 1: INITIAL MODEL: model1&lt;-lm(ANXIETY_TRAIT~SEX+AGE+PROVINCE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model1) # YES significative p-value &lt; 2.2e-16 # Model 2 eliminates EDUCATION: model2&lt;-lm(ANXIETY_TRAIT~SEX+AGE+PROVINCE+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model2) # YES significative p-value &lt; 2.2e-16 # Model 3 eliminates PROVINCE: model3&lt;-lm(ANXIETY_TRAIT~SEX+AGE+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model3) # YES significative p-value &lt; 2.2e-16 # Model 4 eliminates LIVING_WITH_SOMEBODY: model4&lt;-lm(ANXIETY_TRAIT~SEX+AGE+ECONOMIC_INCOME+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model4) # YES significative p-value &lt; 2.2e-16 # Model 5 eliminates SUB PERIODS: model4&lt;-lm(ANXIETY_TRAIT~SEX+AGE+ECONOMIC_INCOME+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY,data=table) summary(model4) # YES significative p-value &lt; 2.2e-16 ################ ## Considering the predictors included in the best-fitted model (i.e., stepwith) in this group, we analyzed the smallest possible linear model for each specific MHS indicator by means of all two-predictor combinations. # We performed all-subsets regression using the regsubsets() function from the leaps package. # We analyzed the three best models for two-predictor subset sizes. leapsbestwith&lt;-regsubsets(ANXIETY_TRAIT~SEX+AGE+ECONOMIC_INCOME+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY,data=table,nbest=3) summary(leapsbestwith) # The best two-predictors model was: ANXIETY_STATE ~ AGE + SUIC_ATTEMPT_HISTORY==no plot(leapsbestwith,scale=&quot;r2&quot;) plot(leapsbestwith,scale=&quot;adjr2&quot;) # First: AGE + SUIC_ATTEMPT_HISTORY (no): besttwowithfirst&lt;-lm(ANXIETY_TRAIT~AGE+SUIC_ATTEMPT_HISTORY,data=table) besttwowithfirst summary(besttwowithfirst) confint(besttwowithfirst) # Second: MENTAL DISORDER (yes) + SUIC_ATTEMPT_HISTORY (no): besttwowithsecond&lt;-lm(ANXIETY_TRAIT~MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY,data=table) besttwowithsecond summary(besttwowithsecond) confint(besttwowithsecond) # Third: SEX (woman) + SUIC_ATTEMPT_HISTORY (no): besttwowiththird&lt;-lm(ANXIETY_TRAIT~SEX+SUIC_ATTEMPT_HISTORY,data=table) besttwowiththird summary(besttwowiththird) confint(besttwowiththird) #### SUICIDAL RISK: # Stepwise Regression fitwith&lt;-lm(SUIC_RISK~SEX+AGE+PROVINCE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) stepwith &lt;- stepAIC(fitwith, trace=TRUE,direction=&quot;both&quot;) stepwith stepwith$anova # display results # Stepwise Model Path # Analysis of Deviance Table # Initial Model: SUIC_RISK ~ SEX + AGE + PROVINCE + EDUCATION + ECONOMIC_INCOME + LIVING_WITH_SOMEBODY + MENTAL_DISORDER_HISTORY + SUIC_ATTEMPT_HISTORY + SUB_PERIODS # Start: AIC = 5732.18 # Final Model: SUIC_RISK ~ SEX + AGE + ECONOMIC_INCOME + MENTAL_DISORDER_HISTORY + SUIC_ATTEMPT_HISTORY # Stepwith: AIC = 5715.52 summary(stepwith) # 95% Confidence interval of best-fitted model: confint(stepwith) # ERROR RATE of best-fitted model: sigma(stepwith)/mean(table$SUIC_RISK) # 0.4417225 # In our multiple regression example, the Residual Standard Error (RSE) or sigma is 13.39 corresponding to 44% error rate. par(mfrow=c(2,2)) # Figure S8 (Supplementary material) plot(stepwith) par(mfrow=c(1,1)) # TABLE 1: # Model 1: INITIAL MODEL: model1&lt;-lm(SUIC_RISK~SEX+AGE+PROVINCE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model1) # YES significative p-value &lt; 2.2e-16 # Model 2 eliminates PROVINCE: model2&lt;-lm(SUIC_RISK~SEX+AGE+EDUCATION+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model2) # YES significative p-value &lt; 2.2e-16 # Model 3 eliminates EDUCATION: model3&lt;-lm(SUIC_RISK~SEX+AGE+ECONOMIC_INCOME+LIVING_WITH_SOMEBODY+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model3) # YES significative p-value &lt; 2.2e-16 # Model 4 eliminates LIVING_WITH_SOMEBODY: model4&lt;-lm(SUIC_RISK~SEX+AGE+ECONOMIC_INCOME+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY+SUB_PERIODS,data=table) summary(model4) # YES significative p-value &lt; 2.2e-16 # Model 5 eliminates SUB PERIODS: model5&lt;-lm(SUIC_RISK~SEX+AGE+ECONOMIC_INCOME+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY,data=table) summary(model5) # YES significative p-value &lt; 2.2e-16 ################ ## Considering the predictors included in the best-fitted model (i.e., stepwith) in this group, we analyzed the smallest possible linear model for each specific MHS indicator by means of all two-predictor combinations. # We performed all-subsets regression using the regsubsets() function from the leaps package. # We analyzed the three best models for two-predictor subset sizes. leapsbestwith&lt;-regsubsets(SUIC_RISK~SEX+AGE+ECONOMIC_INCOME+MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY,data=table,nbest=3) summary(leapsbestwith) # The best two-predictors model was: ANXIETY_STATE ~ AGE + SUIC_ATTEMPT_HISTORY==no plot(leapsbestwith,scale=&quot;r2&quot;) plot(leapsbestwith,scale=&quot;adjr2&quot;) # First: AGE + SUIC_ATTEMPT_HISTORY (no): besttwowithfirst&lt;-lm(SUIC_RISK~AGE+SUIC_ATTEMPT_HISTORY,data=table) besttwowithfirst summary(besttwowithfirst) confint(besttwowithfirst) # Second: MENTAL DISORDER (yes) + SUIC_ATTEMPT_HISTORY (no): besttwowithsecond&lt;-lm(SUIC_RISK~MENTAL_DISORDER_HISTORY+SUIC_ATTEMPT_HISTORY,data=table) besttwowithsecond summary(besttwowithsecond) confint(besttwowithsecond) # Third: ECONOMIC_INCOME (yes) + SUIC_ATTEMPT_HISTORY (no): besttwowiththird&lt;-lm(SUIC_RISK~ECONOMIC_INCOME+SUIC_ATTEMPT_HISTORY,data=table) besttwowiththird summary(besttwowiththird) confint(besttwowiththird) ``` "],["refrences.html", "Chapter 13 Refrences", " Chapter 13 Refrences "]]
