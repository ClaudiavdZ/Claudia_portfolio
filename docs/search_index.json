[["index.html", "My portfolio Chapter 1 Introduction", " My portfolio Claudia van der Zijden 2021-06-20 Chapter 1 Introduction Welkom to my portfolio! In my portfolio I try to give an impression of my programming skills. This is mainly in r but I also have some experience with bash. This portfolio contains a number of chapters with assignments that I have made, it also contains my resume. The last chapter called machine learning is about a tutorial assignment in which I tried to learn more about machine learning. I hope this portfolio will give you a good idea of my skills. For further questions you can always email claudiavanderzijden@hotmail.nl "],["reproducible-research.html", "Chapter 2 Reproducible research", " Chapter 2 Reproducible research C. elegans plate experiment The data for this exercise was kindly supplied by J. Louter (INT/ILC) and was derived from an experiment in which adult C.elegans nematodes were exposed to varying concentrations of different compounds. The variables RawData (the outcome - number of offspring counted as an integer value, after incubation time), compName (the generic name of the compound/chemical), the compConcentration (the concentration of the compound), and the expType are the most important variables in this dataset. A typical analysis with this data would be to run a dose-response analysis using a log-logistic model with estimates for the maximal, the minimal, the IC50 concentration and the slope at IC50. We will not go into the details but a good package to run such computations and create graphs in R is the {drc} package. See: and:. In the exercise below we will create some visualizations using {ggplot2}. Before we start, we will inspect the dataset. We do this by opening it in Excel. When you look at this dataset, a few things stand out. Among other things, there are many tabs with very large tables without an explanation. This makes it difficult for outsiders to use this data. Then we will load the data into rstudio. library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.3 v purrr 0.3.4 ## v tibble 3.1.2 v dplyr 1.0.6 ## v tidyr 1.1.3 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.1 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(readxl) ce_liq_flow_062 &lt;- read_excel(&quot;data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;, sheet = 1) Now we can look at the data types. we will do this for the columns rawData, compName and compConcentration. typeof(ce_liq_flow_062$RawData) ## [1] &quot;double&quot; typeof(ce_liq_flow_062$compName) ## [1] &quot;character&quot; typeof(ce_liq_flow_062$compConcentration) ## [1] &quot;character&quot; You would expect comConcentration to be numeric but as you can see this is character. Now we are going to make a scatter plot of the data. We put compconcentration on the x-axis and DataRaw on the y-axis. We give a different color to the levels of compname and a different shape to the levels of expType. In addition, we ensure that the numbers below the x-axis are rotated 45 degrees so that we can read those. ggplot(data = ce_liq_flow_062, aes(x = compConcentration, y = RawData)) + geom_point(aes(colour = compName, shape = expType)) + scale_x_discrete(guide = guide_axis(angle = 45)) + labs(title = &quot;compConcentration is double&quot;) ## Warning: Removed 5 rows containing missing values (geom_point). If we now look at this plot, you can see that the scale of the x-axis is not linearly distributed. This is probably due to the data type of comcondition. So were going to change it to numeric. Then we will plot the data again. We now use a log10 transformation to improve the distribution of the x-axis. We also use jitter to avoid overlapping data points. ce_liq_flow_062$compConcentration &lt;- as.numeric(as.character(ce_liq_flow_062$compConcentration)) ## Warning: NAs introduced by coercion typeof(ce_liq_flow_062$compConcentration) ## [1] &quot;double&quot; log10_scatter &lt;-ggplot(data = ce_liq_flow_062, aes(x = compConcentration, y = RawData)) + geom_point(position=position_jitter(width=.1,height=0),aes(colour = compName, shape = compName)) + scale_x_discrete(guide = guide_axis(angle = 45))+ labs(title = &quot;compConcentration is numeric&quot;) log10_scatter + scale_x_log10() ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will ## replace the existing scale. ## Warning: Transformation introduced infinite values in continuous x-axis ## Warning: Removed 6 rows containing missing values (geom_point). The positive control for this experiments is naphthale. The negative control for this experiment is S-medium. After reviewing the data, we could proceed with the analysis of the data to find out whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve. To find out, first check whether the data is normally distributed. This can be done with the shapio-wilk test. This can be used to determine whether a parametric or non-parametric test can be used to see if there is a statistically significant difference between the different groups. Finaly we normalize the data for the controlNegative in such a way that the mean value for controlNegative is exactly equal to 1 and all other values are expressed as a fraction thereof. Than we rerun the graph with the normalized data. H.Why would you want to take the step under J? "],["open-peer-review.html", "Chapter 3 Open Peer Review 3.1 Peer revieuw part 1 3.2 Open peer review part 2", " Chapter 3 Open Peer Review In this assisgment w arw goning to find a scientific article ourself, using PubMed or another database or repository. Well use only Open Access articles. When we fond a article we will go over the Transparency Criteria. This is the link to the article I use https://www.biorxiv.org/content/10.1101/2020.10.02.322917v2.full The title of this article is: Leveraging high-throughput screening data and conditional generative adversarial networks to advance predictive toxicology The authors of this article are: Adrian J. Green, Martin J. Mohlenkamp, Jhuma Das, et al. 3.1 Peer revieuw part 1 Study Purpose : the summary briefly explains what is more important to conduct this research Data Availability Statement: not present Data Location: it does describe what the data should look like and there are references to articles that describe how the data was collected. It does not say where you can find the used data. Study Location: there is no information about where the study was conducted in the material and method section Author Review: the details of the authors are not easy to obtain, the names of the authors are at the top of the article but further contact details are not on the page itself. Ethics Statement: the introduction briefly mentions ethics Funding Statement: nothing is said about funding Code Availability: no code is shared in the article 3.2 Open peer review part 2 Next we are going to try to find a article with R code. We do this on the OSF website. We are going to try to get the code working in our r studio. This is the link to the code we will use https://osf.io/gkcn7/ To make this code work we only have to change the way to load the data, out commend the effect sizes and we have to change the column names so the names are not separated by points. You can find the working script in the appendix, chapther 11 It took little effort to get this script working. On a scale of 0 to 5 I would give it a 4 "],["guerrilla-analytics.html", "Chapter 4 Guerrilla analytics 4.1 Daur2 project 4.2 Portfolio project 4.3 Project project", " Chapter 4 Guerrilla analytics In this assignment I cleaned up my projects according to the Guerrilla analytics.The result can be seen below onder. 4.1 Daur2 project Daur 4.2 Portfolio project Portfolio 4.3 Project project Project "],["curriculum-vitae.html", "Chapter 5 Curriculum vitae", " Chapter 5 Curriculum vitae CV "],["mendaly.html", "Chapter 6 Mendaly", " Chapter 6 Mendaly In practice, many use has been made of RNA sequencing (RNA-seq) methods. With RNA-seq, the transcriptional activity of cells can be examined. Next-generation sequencing is used for this. This results in transcriptomics datasets, which makes it difficult for many researchers to process this data and to draw conclusions from it. (Marini, Linke, and Binder 2020). That is why in this project we are working on a way in which researchers without programming knowledge can easily process transcriptome data. The aim is to develop a shiny app where only a GSE/SRA number needs to be entered, after which an analysis is performed automatically. This saves researchers a lot of time. To make this possible, we use salmon and DESeq2 among others. A GSE number can be used to search the SRA database for a dataset in the form of a fastq file. This fastq file can be used in Salmon. Salmon is a package that quantifies the data.(Patro et al. 2017) . When quantifying the data, it is determined how many of the parts of the data correspond to the reference data. Because Salmon takes small pieces of the data for testing, it works much faster and takes much less memory to perform this analysis than other methods. The analyzed data that then comes from the salmon analysis will be further investigated using deseq2. The DESeq2 package provides methods to test for differential expression using negative binomial generalized linear models. (rnaseq?) A sumerised experiment is then made of this data. (sumexp?) A sumerised experiment is a container where data is stored in different dimensions. It contains row data, it contains information about the genes, and the cold data also contains information about the samples. Finally, the sumerised experiment also contains the number of counts. A sumerised experiment will serve as input for the ISEE shiny app. This ISEE app has been specially developed for analyzing transcriptome data (Lun et al. 2018) In this app not only the count data are processed, but also the row data and the cold data are displayed in graphs. This makes the analysis very comprehensive. In our project we also want to pay attention to a new shiny app. Although the ISEE app is a well-functioning app, there are a number of things that we miss. For example, we think that the app is not clear enough because all graphs are plotted on one page without further additions. In addition, we miss a way in which you can enter the GSE/SRA number. So you still need some programming skills to use this app. Ultimately, it would be nice if you only had to fill in a dataset and you would then receive a message as soon as the analysis is ready. At the moment this analysis is ready, it would be nice if a ready-made report could be downloaded with the result processing and statistical tests in it. Looking ahead "],["relational-databases.html", "Chapter 7 Relational databases", " Chapter 7 Relational databases TIPS Be aware, the flu and dengue data contains metadata that should be stripped from the data on load. Think of a way to create valid country names that fit with the gapminder data. Remember (!) that in the end, this assignment needs to be reported by a .Rmd file for your portfolio. So save what you are doing, save your SQL scripts, make screenshots if you want, and in general design a clear and attractive report in RMarkdown to showcase your SQL/database-skills in your portfolio. You may be sending this to propspective employers in the future! (also, the portfolio is what we as teachers will be grading. But definitely think about the future rather than only about passing the course) Assignment Load the flu (./data/flu_data.csv), the dengue (./data/dengue_data.csv) and the gapminder ({dslabs} package) into three separate dataframes in R Check if they are in the right shape. Is the data in the tidy format? If not change the format to tidy Change the country and date variables of the three tables so that they coincide in terms of data type, class and values Store the three tables as separate (so six in total) .csv and .rds files. In Dbeaver create a new PostgreSQL database workflowsdb Using RPostgreSQL, insert the tables into the database. Inspect the contents of the tables with SQL (in DBeaver) and save the SQL script. Inspect the contents of the tables with dplyr (in R) and save a RMarkdown showing what you are doing. Load the gapminder data in R and change the dataframe in such as way that you could join it to dengue and flue. Save this clean gapminder data in the workflowsdb database Perform some joins (your choice) with SQL (can be done in DBeaver or with dplyr. Generate a joined table, and export this from the database to R. Show some descriptive statistics with this table, and at least 3 visualisations using ggplot2. show all of your actions in this assignment in a Rmd file, perhaps with pictures and provide text explaining and showcasing your skills. library(tidyverse) library(dslabs) ## ## Attaching package: &#39;dslabs&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## gapminder gapminder &lt;- as_tibble(gapminder) flu_data&lt;- read.csv(url(&quot;https://raw.githubusercontent.com/ClaudiavdZ/tlsc-dsfb26v-20_workflows/main/data/flu_data.csv&quot;), skip = 11) flu_data &lt;- as_tibble(flu_data) dengue_data&lt;- read.csv(url(&quot;https://raw.githubusercontent.com/ClaudiavdZ/tlsc-dsfb26v-20_workflows/main/data/dengue_data.csv&quot;), skip = 11) write.table(dengue_data , file = &quot;dengu_data.csv&quot;) write.table(dengue_data , file = &quot;dengu_data.RDS&quot;) write.table(flu_data , file = &quot;flu_data.csv&quot;) write.table(flu_data , file = &quot;flu_data.RDS&quot;) write.table(gapminder , file = &quot;gapminder.csv&quot;) write.table(gapminder , file = &quot;gapminder.RDS&quot;) library(DBI) con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;myfirstdb&quot;, host=&quot;localhost&quot;, port=&quot;5432&quot;, user=&quot;postgres&quot;, password=&quot;Veroni36&quot;) dbListTables(con) ## [1] &quot;test&quot; &quot;gapminder&quot; &quot;flu_data&quot; &quot;dengue_data&quot; #dbWriteTable(con, &quot;dengue_data&quot;, dengue_data) #dbWriteTable(con, &quot;flu_data&quot;, flu_data) #dbWriteTable(con, &quot;gapminder&quot;, gapminder) # library(janitor) # gapminder_usd &lt;- as.data.frame(t(gapminder)) # gapminder_usd &lt;- gapminder_usd %&gt;% row_to_names(row_number = 1) flu_usd &lt;- gather( flu_data, key = &quot;country&quot;, value = &quot;flu&quot;, Argentina:Uruguay ) #seperate year from month and day flu_usd &lt;- separate(flu_usd, Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) #count sum of flu flu_usd &lt;- aggregate(flu_usd$flu, by=list(year=flu_usd$year, country=flu_usd$country), FUN=sum) flu_usd &lt;- flu_usd %&gt;% rename(flu = x) flu_usd$year &lt;- as.integer(flu_usd$year) dengue_usd &lt;- gather( dengue_data, key = &quot;country&quot;, value = &quot;dengue&quot;, Argentina:Venezuela ) dengue_usd &lt;- separate(dengue_usd, Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) dengue_usd &lt;- aggregate(dengue_usd$dengue, by=list(year=dengue_usd$year, country=dengue_usd$country), FUN=sum) dengue_usd &lt;- dengue_usd %&gt;% rename(dengue = x) dengue_usd$year &lt;- as.integer(dengue_usd$year) alltogether &lt;- left_join(flu_usd, gapminder, by = c(&quot;country&quot;, &quot;year&quot;)) alltogether &lt;- left_join(alltogether, dengue_usd , by = c(&quot;country&quot;, &quot;year&quot;)) #infant_mortelity firtelety life expantie door flu and dengue in verschillende jaren in verschillende landen #en beetje statestiek flu_plot &lt;- function(dataframe, land){ dataframe %&gt;% filter(country == land) %&gt;% ggplot(aes(x = year, y = flu)) + geom_line() + geom_point() } flu_plot(alltogether,&quot;Netherlands&quot;) ## Warning: Removed 2 row(s) containing missing values (geom_path). ## Warning: Removed 2 rows containing missing values (geom_point). alltogether %&gt;% filter(country == &quot;Argentina&quot;) %&gt;% ggplot() + geom_line(aes(y = dengue,x=year, colour = &quot;green&quot;),) + geom_line(aes(y = flu,x=year, colour = &quot;red&quot;)) ## Warning: Removed 2 row(s) containing missing values (geom_path). ## Warning: Removed 2 row(s) containing missing values (geom_path). ggplot(data = alltogether, aes(x = continent, y = flu)) + geom_boxplot(aes(fill = continent)) ## Warning: Removed 72 rows containing non-finite values (stat_boxplot). shapiro.test(alltogether$fertility) ## ## Shapiro-Wilk normality test ## ## data: alltogether$fertility ## W = 0.84528, p-value &lt; 2.2e-16 shapiro.test(alltogether$flu) ## ## Shapiro-Wilk normality test ## ## data: alltogether$flu ## W = 0.70363, p-value &lt; 2.2e-16 shapiro.test(alltogether$dengue) ## ## Shapiro-Wilk normality test ## ## data: alltogether$dengue ## W = 0.91218, p-value = 0.0009743 shapiro.test(alltogether$infant_mortality) ## ## Shapiro-Wilk normality test ## ## data: alltogether$infant_mortality ## W = 0.75988, p-value &lt; 2.2e-16 shapiro.test(alltogether$life_expectancy) ## ## Shapiro-Wilk normality test ## ## data: alltogether$life_expectancy ## W = 0.93264, p-value = 9.214e-12 shapiro.test(alltogether$gdp) ## ## Shapiro-Wilk normality test ## ## data: alltogether$gdp ## W = 0.53327, p-value &lt; 2.2e-16 shapiro.test(alltogether$population) ## ## Shapiro-Wilk normality test ## ## data: alltogether$population ## W = 0.74646, p-value &lt; 2.2e-16 "],["my-own-package.html", "Chapter 8 My own package", " Chapter 8 My own package "],["parameters.html", "Chapter 9 Parameters", " Chapter 9 Parameters "],["looking-ahead.html", "Chapter 10 Looking ahead", " Chapter 10 Looking ahead "]]
